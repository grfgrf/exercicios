{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V2 pyspark01 - txtMapReduce - GoogleColab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "z91iSn39PQnv"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grfgrf/exercicios/blob/main/pyspark02-txtMapReduce-GoogleColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91iSn39PQnv"
      },
      "source": [
        "#Download/instalação dependências Spark/Pyspark\n",
        "*primeira run aprox 60segundos\n",
        "\n",
        "1.   apt-get openjdk-8-jdk-headless\n",
        "2.   download spark-3.2.0-bin-hadoop3.2.tgz\n",
        "3.   unzip spark-3.2.0-bin-hadoop3.2    \n",
        "4.   download sherlock.txt\n",
        "5.   pip install findspark (pyspark)\n",
        "6.   set JAVA_HOME e SPARK_HOME paths\n",
        "7.   cria variavel \"spark\" como sparkSession\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp6i4txWWMw-"
      },
      "source": [
        "%%bash\n",
        "#***IFs apenas para o notebook não executar novamente em caso de run all cells.\n",
        "\n",
        "#verifica se openjdk está instalado\n",
        "if (dpkg -l | grep -qw openjdk-8-jdk-headless) then \n",
        "  echo \"Ja instalado - openjdk-8-jdk-headless\" \n",
        "else \n",
        "  apt-get install openjdk-8-jdk-headless -qq > /dev/null | echo \"openjdk-8-jdk-headless - instalado com sucesso\" \n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKWPErew7HHg"
      },
      "source": [
        "%%bash\n",
        "#download spark-hadoop\n",
        "if [ -f \"spark-3.2.0-bin-hadoop3.2.tgz\" ]; then\n",
        "  echo \"Ja baixado - spark-3.2.0-bin-hadoop3.2.tgz\"\n",
        "else \n",
        "  wget -q  https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz | echo \"spark-3.2.0-bin-hadoop3.2.tgz - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lnOz9fC7HQ2"
      },
      "source": [
        "%%bash\n",
        "#extrair spark-hadoop\n",
        "if [ -d \"spark-3.2.0-bin-hadoop3.2\" ]; then\n",
        "  echo \"Ja descompactado - Pasta spark-3.2.0-bin-hadoop3.2\"\n",
        "else \n",
        "  tar xf spark-3.2.0-bin-hadoop3.2.tgz | echo \"spark-3.2.0-bin-hadoop3.2.tgz - descompactado com sucesso\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIRCV5z7Hb0"
      },
      "source": [
        "%%bash\n",
        "#download sherlock.txt\n",
        "if [ -f \"/content/sample_data/sherlock.txt\" ]; then\n",
        "  echo \"Ja baixado - sherlock.txt\"\n",
        "else \n",
        "  wget -q -O /content/sample_data/sherlock.txt https://www.gutenberg.org/files/1661/1661-0.txt | echo \"sherlock.txt - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht_iWNwOPqrt"
      },
      "source": [
        "#usa folder do spark como lib pyspark\n",
        "try:\n",
        "    findspark\n",
        "except NameError:\n",
        "    !pip install -q findspark\n",
        "    import findspark\n",
        "    findspark.init('spark-3.2.0-bin-hadoop3.2')\n",
        "    print(\"findspark - instalado com sucesso \")\n",
        "else:\n",
        "    print(\"já instalado - findspark\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvBZvULP8bV"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcDnw-MsFiQU"
      },
      "source": [
        "#cria spark session \n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhYFUL3jxygD"
      },
      "source": [
        "# TOP 10 palavras com RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEClBJC9w-n_"
      },
      "source": [
        "#Exemplo1 com algumas funções Spark\n",
        "#atencao para unicodes e como limpar caracteres especiais\n",
        "import re\n",
        "\n",
        "#wholeTextFiles lê o arquivo inteiro em uma tupla. lista[(path,texto_inteiro)] \n",
        "rddExemplo1 = spark.sparkContext.wholeTextFiles('/content/sample_data/sherlock.txt')\n",
        "\n",
        "#Trata o texto como no exemplo da documentacao: \\s+\n",
        "rddExemplo1 = rddExemplo1.map(lambda x : re.sub(\"\\s+\",\" \",x[1]))\n",
        "\n",
        "#Split por espaço em branco\n",
        "rddExemplo1 = rddExemplo1.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "#Adiciona coluna com valor 1 para reduce (exemplo MRJob)\n",
        "rddExemplo1 = rddExemplo1.map(lambda x: (x,1)) \n",
        "rddExemplo1.take(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X33OFau2ydi1"
      },
      "source": [
        "#Exemplo1 - Solucao1\n",
        "# RDD final com ReduceByKey (reduz keys iguais e soma values)\n",
        "solucao1 = rddExemplo1.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#TOP 10\n",
        "solucao1.sortBy(lambda x: x[1],ascending=False).take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBHB6EVw-g0v"
      },
      "source": [
        "#Exemplo1 - Solucao2\n",
        "#coleta rdd em lista\n",
        "from operator import itemgetter\n",
        "solucao2 = rddExemplo1.groupByKey().mapValues(len).collect()\n",
        "\n",
        "#top10\n",
        "solucao2.sort(key=itemgetter(1),reverse=True)\n",
        "solucao2[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SoQh-Stx4wC"
      },
      "source": [
        "# TOP 10 palavras com DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBIdhB76RXy5"
      },
      "source": [
        "#Exemplo da documentacao Spark - DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "dfExemplo2 =spark.read.format('text').load('/content/sample_data/sherlock.txt')\n",
        "\n",
        "wordCounts = dfExemplo2.select(explode(split(dfExemplo2.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
        "\n",
        "#top 10\n",
        "wordCounts.filter(wordCounts['word']!=\"\").orderBy(['count'],ascending=False).show(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VbBC5AfzRNT"
      },
      "source": [
        "# TOP 10 palavras com DataFrame + SQLquery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQgWgaeQzWdV"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "rddExemplo3 =spark.read.text('/content/sample_data/sherlock.txt').rdd\n",
        "\n",
        "#Split RDD nos espaços em branco\n",
        "rddExemplo3 = rddExemplo3.flatMap(lambda x: x[0].split(\" \"))\n",
        "\n",
        "#add nome Coluna para converter em Dataframe\n",
        "dfExemplo3 = rddExemplo3.map(Row(\"palavra\")).toDF()\n",
        "\n",
        "#Precisa criar tabela/view temporaria para executar queries SQL\n",
        "dfExemplo3.createOrReplaceTempView('tabelaTemp')\n",
        "\n",
        "#Query \n",
        "queryTop10 = \"\"\"SELECT palavra,\n",
        "                       COUNT(*) as qnt\n",
        "                  FROM tabelaTemp\n",
        "                 WHERE palavra <> \"\"           \n",
        "              GROUP BY palavra\n",
        "              ORDER BY 2 DESC\n",
        "                 LIMIT 10\n",
        "             \"\"\"\n",
        "\n",
        "#Executa query\n",
        "spark.sql(queryTop10).show()\n",
        "\n",
        "#dropa tabela temporaria\n",
        "spark.catalog.dropTempView('tabelaTemp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTqIgdshoSfP"
      },
      "source": [
        "#TOP 10 palavras com DataFrame + Python list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uUzC-J3CyF6"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "#leitura .txt\n",
        "#Dataframe com 1 COL e cada ROW sendo uma linha do .txt\n",
        "dfExemplo4 =spark.read.text('/content/sample_data/sherlock.txt')\n",
        "\n",
        "\n",
        "#coluna \"value\" do Dataframe para python list[]\n",
        "listLivro = list(dfExemplo4.select(\"value\").toPandas()[\"value\"])\n",
        "\n",
        "\n",
        "#Formata list[linhaTxt1,...,linhaTxtN]\n",
        "#   para list[palavra1,...,palavraN]\n",
        "def listPalavras(listLivro):\n",
        "  resultado = []\n",
        "  for paragrafo in listLivro:\n",
        "   [resultado.append(palavra) for palavra in paragrafo.split()]\n",
        "  return resultado    \n",
        "\n",
        "#Conta ocorrencias com collections Counter\n",
        "contaPalavra1 = Counter(listPalavras(listLivro))\n",
        "\n",
        "#TOP 10\n",
        "contaPalavra1.most_common(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}