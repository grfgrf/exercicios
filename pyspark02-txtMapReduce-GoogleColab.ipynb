{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V2 pyspark01 - txtMapReduce - GoogleColab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grfgrf/exercicios/blob/main/pyspark02-txtMapReduce-GoogleColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91iSn39PQnv"
      },
      "source": [
        "#Download/instalação dependências Spark/Pyspark\n",
        "*primeira run aprox 60segundos\n",
        "\n",
        "1.   apt-get openjdk-8-jdk-headless\n",
        "2.   download spark-3.2.0-bin-hadoop3.2.tgz\n",
        "3.   unzip spark-3.2.0-bin-hadoop3.2    \n",
        "4.   download sherlock.txt\n",
        "5.   pip install findspark (pyspark)\n",
        "6.   set JAVA_HOME e SPARK_HOME paths\n",
        "7.   cria variavel \"spark\" como sparkSession\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp6i4txWWMw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418bdae9-5a8d-4d8a-ce3c-968fa4c6251a"
      },
      "source": [
        "%%bash\n",
        "#***IFs apenas para o notebook não executar novamente em caso de run all cells.\n",
        "\n",
        "#verifica se openjdk está instalado\n",
        "if (dpkg -l | grep -qw openjdk-8-jdk-headless) then \n",
        "  echo \"Ja instalado - openjdk-8-jdk-headless\" \n",
        "else \n",
        "  apt-get install openjdk-8-jdk-headless -qq > /dev/null | echo \"openjdk-8-jdk-headless - instalado com sucesso\" \n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk-8-jdk-headless - instalado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKWPErew7HHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90dc3a7c-c6a7-4c3f-8eae-773f04386b35"
      },
      "source": [
        "%%bash\n",
        "#download spark-hadoop\n",
        "if [ -f \"spark-3.2.0-bin-hadoop3.2.tgz\" ]; then\n",
        "  echo \"Ja baixado - spark-3.2.0-bin-hadoop3.2.tgz\"\n",
        "else \n",
        "  wget -q  https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz | echo \"spark-3.2.0-bin-hadoop3.2.tgz - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.2.0-bin-hadoop3.2.tgz - baixado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lnOz9fC7HQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5329f5ed-006b-429d-e453-a75892656019"
      },
      "source": [
        "%%bash\n",
        "#extrair spark-hadoop\n",
        "if [ -d \"spark-3.2.0-bin-hadoop3.2\" ]; then\n",
        "  echo \"Ja descompactado - Pasta spark-3.2.0-bin-hadoop3.2\"\n",
        "else \n",
        "  tar xf spark-3.2.0-bin-hadoop3.2.tgz | echo \"spark-3.2.0-bin-hadoop3.2.tgz - descompactado com sucesso\"\n",
        "fi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.2.0-bin-hadoop3.2.tgz - descompactado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VIRCV5z7Hb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5decbe-b5c6-4b50-f13f-25906eb37fc5"
      },
      "source": [
        "%%bash\n",
        "#download sherlock.txt\n",
        "if [ -f \"/content/sample_data/sherlock.txt\" ]; then\n",
        "  echo \"Ja baixado - sherlock.txt\"\n",
        "else \n",
        "  wget -q -O /content/sample_data/sherlock.txt https://www.gutenberg.org/files/1661/1661-0.txt | echo \"sherlock.txt - baixado com sucesso\"\n",
        "fi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sherlock.txt - baixado com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht_iWNwOPqrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2afd679-948f-4738-e18b-cacdadd51498"
      },
      "source": [
        "#usa folder do spark como lib pyspark\n",
        "try:\n",
        "    findspark\n",
        "except NameError:\n",
        "    !pip install -q findspark\n",
        "    import findspark\n",
        "    findspark.init('spark-3.2.0-bin-hadoop3.2')\n",
        "    print(\"findspark - instalado com sucesso \")\n",
        "else:\n",
        "    print(\"já instalado - findspark\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "findspark - instalado com sucesso \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWvBZvULP8bV"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcDnw-MsFiQU"
      },
      "source": [
        "#cria spark session \n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRUzdhgTQ2lN"
      },
      "source": [
        "#Cria Sessão Spark e variáveis com o .txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhYFUL3jxygD"
      },
      "source": [
        "# TOP 10 palavras com RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEClBJC9w-n_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169855d1-c231-4ccf-ff15-6b1241783592"
      },
      "source": [
        "#Exemplo1 com algumas funções Spark\n",
        "#atencao para unicodes e como limpar caracteres especiais\n",
        "import re\n",
        "\n",
        "#wholeTextFiles lê o arquivo inteiro em uma tupla. lista[(path,texto_inteiro)] \n",
        "rddExemplo1 = spark.sparkContext.wholeTextFiles('/content/sample_data/sherlock.txt')\n",
        "\n",
        "#Trata o texto como no exemplo da documentacao: \\s+\n",
        "rddExemplo1 = rddExemplo1.map(lambda x : re.sub(\"\\s+\",\" \",x[1]))\n",
        "\n",
        "#Split por espaço em branco\n",
        "rddExemplo1 = rddExemplo1.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "#Adiciona coluna com valor 1 para reduce (exemplo MRJob)\n",
        "rddExemplo1 = rddExemplo1.map(lambda x: (x,1)) \n",
        "rddExemplo1.take(5)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\\ufeffThe', 1), ('Project', 1), ('Gutenberg', 1), ('eBook', 1), ('of', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X33OFau2ydi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34863bf9-c523-45f3-fa8e-bee2856ae95d"
      },
      "source": [
        "#Exemplo1 - Solucao1\n",
        "# RDD final com ReduceByKey (reduz keys iguais e soma values)\n",
        "solucao1 = rddExemplo1.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "#TOP 10\n",
        "solucao1.sortBy(lambda x: x[1],ascending=False).take(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBHB6EVw-g0v",
        "outputId": "ebcd3635-efed-4a41-ec05-36add1947d99"
      },
      "source": [
        "#Exemplo1 - Solucao2\n",
        "#coleta rdd em lista\n",
        "from operator import itemgetter\n",
        "solucao2 = rddExemplo1.groupByKey().mapValues(len).collect()\n",
        "\n",
        "#top10\n",
        "solucao2.sort(key=itemgetter(1),reverse=True)\n",
        "solucao2[0:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SoQh-Stx4wC"
      },
      "source": [
        "# TOP 10 palavras com DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBIdhB76RXy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1f99f8-090e-47af-eea5-3eac7dea4703"
      },
      "source": [
        "#Exemplo da documentacao Spark - DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "dfExemplo2 =spark.read.format('text').load('/content/sample_data/sherlock.txt')\n",
        "\n",
        "wordCounts = dfExemplo2.select(explode(split(dfExemplo2.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
        "\n",
        "#top 10\n",
        "wordCounts.filter(wordCounts['word']!=\"\").orderBy(['count'],ascending=False).show(10)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "| the| 5412|\n",
            "| and| 2794|\n",
            "|  of| 2724|\n",
            "|  to| 2702|\n",
            "|   a| 2575|\n",
            "|   I| 2533|\n",
            "|  in| 1706|\n",
            "|that| 1557|\n",
            "| was| 1361|\n",
            "| his| 1096|\n",
            "+----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VbBC5AfzRNT"
      },
      "source": [
        "# TOP 10 palavras com DataFrame + SQLquery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQgWgaeQzWdV",
        "outputId": "82719a7e-0f07-498c-8e0a-72fe93462ca1"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "rddExemplo3 =spark.read.text('/content/sample_data/sherlock.txt').rdd\n",
        "\n",
        "#Split RDD nos espaços em branco\n",
        "rddExemplo3 = rddExemplo3.flatMap(lambda x: x[0].split(\" \"))\n",
        "\n",
        "#add nome Coluna para converter em Dataframe\n",
        "dfExemplo3 = rddExemplo3.map(Row(\"palavra\")).toDF()\n",
        "\n",
        "#Precisa criar tabela/view temporaria para executar queries SQL\n",
        "dfExemplo3.createOrReplaceTempView('tabelaTemp')\n",
        "\n",
        "#Query \n",
        "queryTop10 = \"\"\"SELECT palavra,\n",
        "                       COUNT(*) as qnt\n",
        "                  FROM tabelaTemp\n",
        "                 WHERE palavra <> \"\"           \n",
        "              GROUP BY palavra\n",
        "              ORDER BY 2 DESC\n",
        "                 LIMIT 10\n",
        "             \"\"\"\n",
        "\n",
        "#Executa query\n",
        "spark.sql(queryTop10).show()\n",
        "\n",
        "#dropa tabela temporaria\n",
        "spark.catalog.dropTempView('tabelaTemp')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|palavra| qnt|\n",
            "+-------+----+\n",
            "|    the|5412|\n",
            "|    and|2794|\n",
            "|     of|2724|\n",
            "|     to|2702|\n",
            "|      a|2575|\n",
            "|      I|2533|\n",
            "|     in|1706|\n",
            "|   that|1557|\n",
            "|    was|1361|\n",
            "|    his|1096|\n",
            "+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTqIgdshoSfP"
      },
      "source": [
        "#TOP 10 palavras com DataFrame + Python list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uUzC-J3CyF6",
        "outputId": "bd3227ef-e180-4fc9-aaf5-82066262114a"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "#leitura .txt\n",
        "#Dataframe com 1 COL e cada ROW sendo uma linha do .txt\n",
        "dfExemplo4 =spark.read.text('/content/sample_data/sherlock.txt')\n",
        "\n",
        "\n",
        "#coluna \"value\" do Dataframe para python list[]\n",
        "listLivro = list(dfExemplo4.select(\"value\").toPandas()[\"value\"])\n",
        "\n",
        "\n",
        "#Formata list[linhaTxt1,...,linhaTxtN]\n",
        "#   para list[palavra1,...,palavraN]\n",
        "def listPalavras(listLivro):\n",
        "  resultado = []\n",
        "  for paragrafo in listLivro:\n",
        "   [resultado.append(palavra) for palavra in paragrafo.split()]\n",
        "  return resultado    \n",
        "\n",
        "#Conta ocorrencias com collections Counter\n",
        "contaPalavra1 = Counter(listPalavras(listLivro))\n",
        "\n",
        "#TOP 10\n",
        "contaPalavra1.most_common(10)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 5412),\n",
              " ('and', 2794),\n",
              " ('of', 2724),\n",
              " ('to', 2702),\n",
              " ('a', 2575),\n",
              " ('I', 2533),\n",
              " ('in', 1706),\n",
              " ('that', 1557),\n",
              " ('was', 1361),\n",
              " ('his', 1096)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}